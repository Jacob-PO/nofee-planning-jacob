"""
ë‹¹ê·¼ë§ˆì¼“ íœ´ëŒ€í° ë§¤ì¥ í¬ë¡¤ëŸ¬ (Selenium ë²„ì „)
- Chrome ë¸Œë¼ìš°ì €ë¥¼ ì§ì ‘ ì œì–´
- ë‹¹ê·¼ë§ˆì¼“ ì‚¬ì´íŠ¸ì—ì„œ ë§¤ì¥ ê²€ìƒ‰
- ì‹¤ì‹œê°„ ì§„í–‰ìƒí™© ì¶œë ¥
"""

import time
import re
import random
from datetime import datetime
from pathlib import Path
import pandas as pd
import gspread
from google.oauth2.service_account import Credentials
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.common.exceptions import TimeoutException, NoSuchElementException

class DaangnStoreCrawlerSelenium:
    """ë‹¹ê·¼ë§ˆì¼“ íœ´ëŒ€í° ë§¤ì¥ í¬ë¡¤ëŸ¬ (Selenium)"""

    def __init__(self, google_api_key_path=None, headless=False):
        self.base_path = Path(__file__).parent
        self.output_path = self.base_path / 'output'
        self.output_path.mkdir(exist_ok=True)

        if google_api_key_path is None:
            self.google_api_key_path = Path('/Users/jacob/Desktop/dev/config/google_api_key.json')
        else:
            self.google_api_key_path = Path(google_api_key_path)

        # Chrome ì˜µì…˜ ì„¤ì • (ë´‡ íƒì§€ íšŒí”¼ ê°•í™”)
        self.chrome_options = Options()
        if headless:
            self.chrome_options.add_argument('--headless')
        self.chrome_options.add_argument('--no-sandbox')
        self.chrome_options.add_argument('--disable-dev-shm-usage')
        self.chrome_options.add_argument('--disable-blink-features=AutomationControlled')

        # User-Agent ëœë¤í™”
        user_agents = [
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Safari/605.1.15'
        ]
        self.chrome_options.add_argument(f'user-agent={random.choice(user_agents)}')

        # ì¶”ê°€ ë´‡ íƒì§€ íšŒí”¼ ì˜µì…˜
        self.chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
        self.chrome_options.add_experimental_option('useAutomationExtension', False)
        self.chrome_options.add_argument('--disable-gpu')
        self.chrome_options.add_argument('--lang=ko-KR')

        self.driver = None

        # ì „êµ­ ì§€ì—­ (ì„œìš¸/ìˆ˜ë„ê¶Œ ì„¸ë¶„í™”)
        self.regions = [
            # ì„œìš¸ 25ê°œ êµ¬ (ì„¸ë¶„í™”)
            'ì„œìš¸ ê°•ë‚¨êµ¬', 'ì„œìš¸ ê°•ë™êµ¬', 'ì„œìš¸ ê°•ë¶êµ¬', 'ì„œìš¸ ê°•ì„œêµ¬',
            'ì„œìš¸ ê´€ì•…êµ¬', 'ì„œìš¸ ê´‘ì§„êµ¬', 'ì„œìš¸ êµ¬ë¡œêµ¬', 'ì„œìš¸ ê¸ˆì²œêµ¬',
            'ì„œìš¸ ë…¸ì›êµ¬', 'ì„œìš¸ ë„ë´‰êµ¬', 'ì„œìš¸ ë™ëŒ€ë¬¸êµ¬', 'ì„œìš¸ ë™ì‘êµ¬',
            'ì„œìš¸ ë§ˆí¬êµ¬', 'ì„œìš¸ ì„œëŒ€ë¬¸êµ¬', 'ì„œìš¸ ì„œì´ˆêµ¬', 'ì„œìš¸ ì„±ë™êµ¬',
            'ì„œìš¸ ì„±ë¶êµ¬', 'ì„œìš¸ ì†¡íŒŒêµ¬', 'ì„œìš¸ ì–‘ì²œêµ¬', 'ì„œìš¸ ì˜ë“±í¬êµ¬',
            'ì„œìš¸ ìš©ì‚°êµ¬', 'ì„œìš¸ ì€í‰êµ¬', 'ì„œìš¸ ì¢…ë¡œêµ¬', 'ì„œìš¸ ì¤‘êµ¬', 'ì„œìš¸ ì¤‘ë‘êµ¬',

            # ê²½ê¸° ë‚¨ë¶€ (ì„œìš¸ ì¸ì ‘ ì§€ì—­)
            'ê²½ê¸° ì„±ë‚¨', 'ê²½ê¸° ìˆ˜ì›', 'ê²½ê¸° ì•ˆì–‘', 'ê²½ê¸° ì•ˆì‚°', 'ê²½ê¸° ìš©ì¸',
            'ê²½ê¸° ê´‘ëª…', 'ê²½ê¸° ê³¼ì²œ', 'ê²½ê¸° ì˜ì™•', 'ê²½ê¸° êµ°í¬', 'ê²½ê¸° ì‹œí¥',
            'ê²½ê¸° ë¶€ì²œ', 'ê²½ê¸° ê´‘ì£¼', 'ê²½ê¸° í•˜ë‚¨', 'ê²½ê¸° í™”ì„±', 'ê²½ê¸° ì˜¤ì‚°',

            # ê²½ê¸° ë¶ë¶€
            'ê²½ê¸° ê³ ì–‘', 'ê²½ê¸° íŒŒì£¼', 'ê²½ê¸° ì˜ì •ë¶€', 'ê²½ê¸° ì–‘ì£¼', 'ê²½ê¸° ë™ë‘ì²œ',
            'ê²½ê¸° ë‚¨ì–‘ì£¼', 'ê²½ê¸° êµ¬ë¦¬', 'ê²½ê¸° í¬ì²œ', 'ê²½ê¸° ì—°ì²œ', 'ê²½ê¸° ê°€í‰',

            # ê²½ê¸° ë™ë¶€
            'ê²½ê¸° ì´ì²œ', 'ê²½ê¸° ì—¬ì£¼', 'ê²½ê¸° ì–‘í‰',

            # ê²½ê¸° ì„œë¶€
            'ê²½ê¸° ê¹€í¬', 'ê²½ê¸° ì¸ì²œ', 'ì¸ì²œ ì¤‘êµ¬', 'ì¸ì²œ ë™êµ¬', 'ì¸ì²œ ë‚¨êµ¬',
            'ì¸ì²œ ì—°ìˆ˜êµ¬', 'ì¸ì²œ ë‚¨ë™êµ¬', 'ì¸ì²œ ë¶€í‰êµ¬', 'ì¸ì²œ ê³„ì–‘êµ¬',
            'ì¸ì²œ ì„œêµ¬', 'ì¸ì²œ ê°•í™”êµ°', 'ì¸ì²œ ì˜¹ì§„êµ°',

            # ê¸°íƒ€ ê´‘ì—­ì‹œ
            'ë¶€ì‚°', 'ëŒ€êµ¬', 'ê´‘ì£¼', 'ëŒ€ì „', 'ìš¸ì‚°', 'ì„¸ì¢…',

            # ê¸°íƒ€ ë„
            'ê°•ì›', 'ì¶©ë¶', 'ì¶©ë‚¨', 'ì „ë¶', 'ì „ë‚¨', 'ê²½ë¶', 'ê²½ë‚¨', 'ì œì£¼'
        ]

        # ê²€ìƒ‰ í‚¤ì›Œë“œ (ë…¸í”¼ "ë™ë„¤ì„±ì§€" ì»¨ì…‰ ê¸°ë°˜)
        self.keywords = [
            # í•µì‹¬: ë™ë„¤ íŒë§¤ì  í‚¤ì›Œë“œ (ë…¸í”¼ì˜ ë³¸ì§ˆ)
            'íœ´ëŒ€í°ë§¤ì¥', 'íœ´ëŒ€í°ì„±ì§€', 'ìŠ¤ë§ˆíŠ¸í°ë§¤ì¥', 'í°ë§¤ì¥',
            'íœ´ëŒ€í°ê°€ê²Œ', 'í•¸ë“œí°ê°€ê²Œ', 'ë™ë„¤íœ´ëŒ€í°ë§¤ì¥',

            # íŒë§¤/ê°œí†µ í‚¤ì›Œë“œ (O2O ì‹ ë¢° ê¸°ë°˜)
            'íœ´ëŒ€í°íŒë§¤', 'íœ´ëŒ€í°ëŒ€ë¦¬ì ', 'í•¸ë“œí°ë§¤ì¥', 'í•¸ë“œí°íŒë§¤',
            'ìŠ¤ë§ˆíŠ¸í°íŒë§¤', 'íœ´ëŒ€í°ê°œí†µ', 'ê¸°ê¸°ë³€ê²½', 'ë²ˆí˜¸ì´ë™',

            # ê¸°ê¸°ë³„ í‚¤ì›Œë“œ (ì£¼ìš” ê²€ìƒ‰ ìˆ˜ìš”)
            'ì•„ì´í°', 'ê°¤ëŸ­ì‹œ', 'ì•„ì´í°ë§¤ì¥', 'ê°¤ëŸ­ì‹œë§¤ì¥',
            'ì•„ì´í°íŒë§¤', 'ê°¤ëŸ­ì‹œíŒë§¤',

            # ì‹ ë¢°/í›„ê¸° í‚¤ì›Œë“œ (ë™ë„¤ì„±ì§€ í•µì‹¬)
            'íœ´ëŒ€í°ë§¤ì¥ì¶”ì²œ', 'ë¯¿ì„ë§Œí•œíœ´ëŒ€í°ë§¤ì¥', 'ì•ˆì „í•œê°œí†µ',
            'íœ´ëŒ€í°ì„±ì§€í›„ê¸°', 'íœ´ëŒ€í°ë§¤ì¥í›„ê¸°',
        ]

        self.results = []

    def init_driver(self):
        """Chrome ë“œë¼ì´ë²„ ì´ˆê¸°í™”"""
        try:
            print("ğŸŒ Chrome ë¸Œë¼ìš°ì € ì‹œì‘ ì¤‘...")
            self.driver = webdriver.Chrome(options=self.chrome_options)
            self.driver.set_window_size(1920, 1080)

            # WebDriver íƒì§€ ìš°íšŒ ìŠ¤í¬ë¦½íŠ¸ ì£¼ì…
            self.driver.execute_cdp_cmd('Page.addScriptToEvaluateOnNewDocument', {
                'source': '''
                    Object.defineProperty(navigator, 'webdriver', {
                        get: () => undefined
                    });
                '''
            })

            print("âœ… Chrome ë¸Œë¼ìš°ì € ì‹œì‘ ì™„ë£Œ (ë´‡ íƒì§€ íšŒí”¼ í™œì„±í™”)")
            return True
        except Exception as e:
            print(f"âŒ Chrome ë“œë¼ì´ë²„ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
            print("   chromedriverê°€ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
            return False

    def close_driver(self):
        """Chrome ë“œë¼ì´ë²„ ì¢…ë£Œ"""
        if self.driver:
            self.driver.quit()
            print("ğŸ”´ Chrome ë¸Œë¼ìš°ì € ì¢…ë£Œ")

    def search_daangn_stores(self, keyword, region=None):
        """ë‹¹ê·¼ë§ˆì¼“ì—ì„œ ë§¤ì¥ ê²€ìƒ‰"""
        try:
            search_query = f"{region} {keyword}" if region else keyword
            print(f"  ğŸ” ê²€ìƒ‰: {search_query}")

            # Google ê²€ìƒ‰
            google_url = f"https://www.google.com/search?q=ë‹¹ê·¼ë§ˆì¼“+{search_query}+site:daangn.com"
            self.driver.get(google_url)

            # í˜ì´ì§€ ë¡œë“œ ëŒ€ê¸° ì‹œê°„ ëœë¤í™” (3-5ì´ˆ)
            wait_time = random.uniform(3, 5)
            time.sleep(wait_time)

            # ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ë‹¹ê·¼ë§ˆì¼“ ë§í¬ ìˆ˜ì§‘
            daangn_links = []
            try:
                # ë‹¤ì–‘í•œ CSS ì…€ë ‰í„°ë¡œ ì‹œë„
                selectors = [
                    'a[href*="daangn.com"]',
                    'div.g a',
                    'a[jsname]',
                    'a',
                ]

                all_links = []
                for selector in selectors:
                    elements = self.driver.find_elements(By.CSS_SELECTOR, selector)
                    for elem in elements:
                        url = elem.get_attribute('href')
                        if url and 'daangn.com' in url:
                            all_links.append(url)

                # ì¤‘ë³µ ì œê±°
                all_links = list(set(all_links))

                print(f"    ğŸ“Œ ì´ {len(all_links)}ê°œ daangn.com ë§í¬ ë°œê²¬")

                # local-profile ë§í¬ë§Œ í•„í„°ë§ (ê²€ìƒ‰ ê²°ê³¼ í˜ì´ì§€ ì œì™¸)
                for url in all_links:
                    if 'local-profile' in url or 'business-profile' in url:
                        # ê²€ìƒ‰ ê²°ê³¼ í˜ì´ì§€ëŠ” ì œì™¸ (?in= ì™€ search= ê°€ ìˆëŠ” ê²½ìš°)
                        if '?in=' in url and 'search=' in url:
                            continue
                        if url not in daangn_links:
                            daangn_links.append(url)
                            print(f"    âœ… í”„ë¡œí•„ ë§í¬ ë°œê²¬: {url[:80]}...")

                # í”„ë¡œí•„ ë§í¬ê°€ ì—†ìœ¼ë©´ ëª¨ë“  ë‹¹ê·¼ë§ˆì¼“ ë§í¬ ì¶œë ¥ (ë””ë²„ê¹…ìš©)
                if not daangn_links and all_links:
                    print(f"    â„¹ï¸  ë°œê²¬ëœ ë§í¬ ìƒ˜í”Œ:")
                    for url in all_links[:5]:
                        print(f"      - {url[:100]}")

            except Exception as e:
                print(f"    âš ï¸  ê²€ìƒ‰ ê²°ê³¼ íŒŒì‹± ì‹¤íŒ¨: {str(e)}")
                import traceback
                traceback.print_exc()

            return daangn_links

        except Exception as e:
            print(f"    âŒ ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}")
            import traceback
            traceback.print_exc()
            return []

    def extract_store_info_from_page(self, url):
        """ë‹¹ê·¼ë§ˆì¼“ í˜ì´ì§€ì—ì„œ ë§¤ì¥ ì •ë³´ ì¶”ì¶œ (ê°œë³„ ë§¤ì¥ í˜ì´ì§€ë§Œ)"""
        try:
            # ê²€ìƒ‰ ê²°ê³¼ í˜ì´ì§€ëŠ” ê±´ë„ˆë›°ê¸°
            if '?in=' in url and 'search=' in url:
                print(f"    â­ï¸  ê²€ìƒ‰ ê²°ê³¼ í˜ì´ì§€ - ê±´ë„ˆëœ€")
                return None

            print(f"    ğŸ“„ í˜ì´ì§€ ë¶„ì„ ì¤‘...")
            self.driver.get(url)
            time.sleep(3)

            # ê°œë³„ ë§¤ì¥ í˜ì´ì§€ë§Œ ì²˜ë¦¬
            if False:  # ê²€ìƒ‰ ê²°ê³¼ í˜ì´ì§€ ì²˜ë¦¬ ë¹„í™œì„±í™”
                # ê²€ìƒ‰ ê²°ê³¼ í˜ì´ì§€ - ì—¬ëŸ¬ ë§¤ì¥ ë¦¬ìŠ¤íŠ¸
                print(f"      ğŸ“‹ ê²€ìƒ‰ ê²°ê³¼ í˜ì´ì§€ - ë§¤ì¥ ë¦¬ìŠ¤íŠ¸ ì¶”ì¶œ")

                try:
                    # ë§¤ì¥ ë¦¬ìŠ¤íŠ¸ ì•„ì´í…œ ì°¾ê¸° (XPath ì‚¬ìš©)
                    store_items = self.driver.find_elements(By.XPATH, '//ul/li')

                    print(f"      ğŸ“Œ {len(store_items)}ê°œ ì•„ì´í…œ ë°œê²¬")

                    count = 0
                    for idx, item in enumerate(store_items[:20], 1):  # ìƒìœ„ 20ê°œë§Œ
                        try:
                            # ë§í¬ ì°¾ê¸°
                            try:
                                link_elem = item.find_element(By.CSS_SELECTOR, 'a[href*="local-profile"]')
                                store_url = link_elem.get_attribute('href')
                            except:
                                continue

                            # ì „ì²´ í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
                            item_text = item.text

                            # ë””ë²„ê¹…: ì²« ë²ˆì§¸ ì•„ì´í…œ ì¶œë ¥
                            if idx == 1 and item_text:
                                print(f"      [ë””ë²„ê·¸] ì²« ë²ˆì§¸ ì•„ì´í…œ í…ìŠ¤íŠ¸:\n{item_text[:200]}")

                            # ì „í™”ë²ˆí˜¸ ë¨¼ì € í™•ì¸
                            phone_pattern = r'010-?\d{3,4}-?\d{4}'
                            phones = re.findall(phone_pattern, item_text)

                            if not phones:
                                continue

                            # ë§¤ì¥ëª… ì¶”ì¶œ
                            store_name = item_text.split('\n')[0].strip()

                            if not store_name or len(store_name) > 50:
                                continue

                            # ì§€ì—­ ì¶”ì¶œ
                            from urllib.parse import unquote
                            region = 'ì§€ì—­ ë¯¸í™•ì¸'
                            if '?in=' in url:
                                region_param = url.split('?in=')[1].split('&')[0]
                                region = unquote(region_param)

                            if phones:
                                # ì „í™”ë²ˆí˜¸ ì •ê·œí™”
                                normalized_phones = []
                                for phone in phones:
                                    digits = re.sub(r'[^0-9]', '', phone)
                                    if len(digits) == 10:
                                        formatted = f"{digits[:3]}-{digits[3:6]}-{digits[6:]}"
                                    elif len(digits) == 11:
                                        formatted = f"{digits[:3]}-{digits[3:7]}-{digits[7:]}"
                                    else:
                                        formatted = phone
                                    normalized_phones.append(formatted)

                                unique_phones = list(set(normalized_phones))

                                results.append({
                                    'store_name': store_name,
                                    'phones': unique_phones,
                                    'region': region,
                                    'url': store_url
                                })
                                count += 1
                                print(f"        [{count}] {store_name}: {unique_phones[0]}")

                        except Exception as e:
                            continue

                    print(f"      âœ… ì´ {count}ê°œ ë§¤ì¥ ì „í™”ë²ˆí˜¸ ì¶”ì¶œ")

                except Exception as e:
                    print(f"      âš ï¸  ë¦¬ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
                    import traceback
                    traceback.print_exc()

                return results if results else None

            else:
                # ê°œë³„ ë§¤ì¥ í˜ì´ì§€
                page_text = self.driver.find_element(By.TAG_NAME, 'body').text

                # ë§¤ì¥ëª… ì¶”ì¶œ - XPath ì‚¬ìš©
                store_name = "ë§¤ì¥ëª… ë¯¸í™•ì¸"
                try:
                    # XPathë¡œ ë§¤ì¥ëª… ì°¾ê¸°
                    store_element = self.driver.find_element(By.XPATH, '/html/body/main/div[1]/div[2]/div[1]/h1')
                    store_name = store_element.text.strip()
                    print(f"    âœ… ë§¤ì¥ëª…: {store_name}")
                except:
                    # ëŒ€ì²´ ë°©ë²•: h1 íƒœê·¸ì—ì„œ ì°¾ê¸°
                    try:
                        title_elements = self.driver.find_elements(By.CSS_SELECTOR, 'h1')
                        if title_elements:
                            store_name = title_elements[0].text.strip()
                    except:
                        pass

                # URLì—ì„œë„ ì‹œë„
                if store_name == "ë§¤ì¥ëª… ë¯¸í™•ì¸":
                    url_parts = url.split('local-profile/')
                    if len(url_parts) > 1:
                        store_part = url_parts[1].split('-')[0]
                        from urllib.parse import unquote
                        store_name = unquote(store_part)

                # ì „í™”ë²ˆí˜¸ ì¶”ì¶œ
                phone_pattern = r'010-?\d{3,4}-?\d{4}'
                phones = re.findall(phone_pattern, page_text)

                # ì •ê·œí™”
                normalized_phones = []
                for phone in phones:
                    digits = re.sub(r'[^0-9]', '', phone)
                    if len(digits) == 10:
                        formatted = f"{digits[:3]}-{digits[3:6]}-{digits[6:]}"
                    elif len(digits) == 11:
                        formatted = f"{digits[:3]}-{digits[3:7]}-{digits[7:]}"
                    else:
                        formatted = phone
                    normalized_phones.append(formatted)

                # ì¤‘ë³µ ì œê±°
                unique_phones = list(set(normalized_phones))

                # ì§€ì—­ ì¶”ì¶œ (ìƒì„¸ - ì‹œ/ë„ + êµ¬/êµ°)
                region = 'ì§€ì—­ ë¯¸í™•ì¸'

                # URLì—ì„œ ì§€ì—­ ì¶”ì¶œ ì‹œë„
                if '?in=' in url:
                    from urllib.parse import unquote
                    region_param = url.split('?in=')[1].split('&')[0]
                    region = unquote(region_param)

                # í˜ì´ì§€ í…ìŠ¤íŠ¸ì—ì„œ ìƒì„¸ ì§€ì—­ ì¶”ì¶œ
                if region == 'ì§€ì—­ ë¯¸í™•ì¸':
                    # "ì„œìš¸ ê°•ë‚¨êµ¬", "ë¶€ì‚° í•´ìš´ëŒ€êµ¬" ê°™ì€ íŒ¨í„´ ì°¾ê¸°
                    region_pattern = r'(ì„œìš¸|ë¶€ì‚°|ëŒ€êµ¬|ì¸ì²œ|ê´‘ì£¼|ëŒ€ì „|ìš¸ì‚°|ì„¸ì¢…|ê²½ê¸°|ê°•ì›|ì¶©ë¶|ì¶©ë‚¨|ì „ë¶|ì „ë‚¨|ê²½ë¶|ê²½ë‚¨|ì œì£¼)\s*([ê°€-í£]+[ì‹œêµ°êµ¬])'
                    region_match = re.search(region_pattern, page_text)
                    if region_match:
                        region = f"{region_match.group(1)} {region_match.group(2)}"
                    else:
                        # ë‹¨ìˆœíˆ ì‹œ/ë„ë§Œ ì°¾ê¸°
                        for r in self.regions:
                            if r in page_text:
                                region = r
                                break

                if unique_phones:
                    print(f"    âœ… ë§¤ì¥: {store_name}, ì „í™”ë²ˆí˜¸: {len(unique_phones)}ê°œ, ì§€ì—­: {region}")
                    return [{
                        'store_name': store_name,
                        'phones': unique_phones,
                        'region': region,
                        'url': url
                    }]
                else:
                    print(f"    âš ï¸  ì „í™”ë²ˆí˜¸ ì—†ìŒ")
                    return None

        except Exception as e:
            print(f"    âŒ í˜ì´ì§€ ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
            import traceback
            traceback.print_exc()
            return None

    def save_intermediate_results(self, results, search_count):
        """ì¤‘ê°„ ê²°ê³¼ ì €ì¥"""
        if not results:
            return

        try:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = self.output_path / f'daangn_stores_intermediate_{search_count}searches_{timestamp}.csv'

            df = pd.DataFrame(results)
            df.to_csv(filename, index=False, encoding='utf-8-sig')
            print(f"    ğŸ’¾ ì¤‘ê°„ ì €ì¥ ì™„ë£Œ: {len(results)}ê°œ ë§¤ì¥ â†’ {filename.name}")
        except Exception as e:
            print(f"    âš ï¸  ì¤‘ê°„ ì €ì¥ ì‹¤íŒ¨: {str(e)}")

    def crawl(self, max_searches=30, save_interval=50):
        """í¬ë¡¤ë§ ì‹¤í–‰ (ìë™ ì¬ì‹œì‘ ë° ì¤‘ê°„ ì €ì¥ ê¸°ëŠ¥ í¬í•¨)"""
        print("=" * 80)
        print("ğŸ¥• ë‹¹ê·¼ë§ˆì¼“ íœ´ëŒ€í° ë§¤ì¥ í¬ë¡¤ëŸ¬ (Selenium)")
        print("=" * 80)

        all_results = []
        visited_urls = set()
        search_count = 0
        retry_count = 0
        max_retries = 3

        # ì§€ì—­ë³„ í‚¤ì›Œë“œ ì¡°í•©
        for region in self.regions:
            if search_count >= max_searches:
                break

            for keyword in self.keywords:
                if search_count >= max_searches:
                    break

                print(f"\n[{search_count + 1}/{max_searches}] ğŸ” {region} {keyword}")

                # ìë™ ì¬ì‹œì‘ ë¡œì§
                while retry_count < max_retries:
                    try:
                        # ë“œë¼ì´ë²„ê°€ ì—†ê±°ë‚˜ ì—°ê²°ì´ ëŠì–´ì§„ ê²½ìš° ì¬ì´ˆê¸°í™”
                        if self.driver is None:
                            print("    ğŸ”„ ë“œë¼ì´ë²„ ì¬ì´ˆê¸°í™” ì¤‘...")
                            if not self.init_driver():
                                retry_count += 1
                                print(f"    âš ï¸  ì¬ì‹œë„ {retry_count}/{max_retries}")
                                time.sleep(5)
                                continue
                            retry_count = 0  # ì„±ê³µì‹œ ë¦¬ì…‹

                        # ê²€ìƒ‰
                        daangn_links = self.search_daangn_stores(keyword, region)

                        if not daangn_links:
                            print(f"    âš ï¸  ë§í¬ ì—†ìŒ")
                            search_count += 1
                            break

                        print(f"    ğŸ“ {len(daangn_links)}ê°œ ë§í¬ ë°œê²¬")

                        # ê° ë§í¬ ë°©ë¬¸í•˜ì—¬ ì •ë³´ ì¶”ì¶œ
                        for link in daangn_links:
                            if link in visited_urls:
                                continue

                            visited_urls.add(link)
                            store_info_list = self.extract_store_info_from_page(link)

                            # ê²°ê³¼ê°€ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜ë¨ (ê²€ìƒ‰ ê²°ê³¼ í˜ì´ì§€ì¸ ê²½ìš° ì—¬ëŸ¬ ë§¤ì¥)
                            if store_info_list:
                                for store_info in store_info_list:
                                    if store_info.get('phones'):
                                        for phone in store_info['phones']:
                                            result = {
                                                'ì§€ì—­ëª…': store_info['region'],
                                                'ë§¤ì¥ëª…': store_info['store_name'],
                                                'ì§€ì—­ëª…_ë§¤ì¥ëª…': f"{store_info['region']}_{store_info['store_name']}",
                                                'ì „í™”ë²ˆí˜¸': phone,
                                                'ë§í¬': store_info['url']
                                            }
                                            all_results.append(result)
                                            print(f"      ğŸ’¾ ì €ì¥: {result['ë§¤ì¥ëª…']} ({phone})")

                            time.sleep(5)  # ìš”ì²­ ê°„ê²© (2ì´ˆ â†’ 5ì´ˆ)

                        search_count += 1

                        # ì¤‘ê°„ ì €ì¥ (ì¼ì • ê°œìˆ˜ë§ˆë‹¤)
                        if search_count % save_interval == 0:
                            print(f"\nğŸ“¦ ì¤‘ê°„ ì €ì¥ ì‹œì  ({search_count}ë²ˆ ê²€ìƒ‰ ì™„ë£Œ)")
                            self.save_intermediate_results(all_results, search_count)

                        # ê²€ìƒ‰ ê°„ê²©ì„ 10-15ì´ˆë¡œ ëœë¤í•˜ê²Œ ì¦ê°€ (Google CAPTCHA íšŒí”¼)
                        wait_time = random.uniform(10, 15)
                        print(f"    â³ {wait_time:.1f}ì´ˆ ëŒ€ê¸° ì¤‘... (Google ë´‡ íƒì§€ íšŒí”¼)")
                        time.sleep(wait_time)

                        # ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ë©´ ë£¨í”„ íƒˆì¶œ
                        break

                    except Exception as e:
                        # ë“œë¼ì´ë²„ ì—°ê²° ì˜¤ë¥˜ ë°œìƒì‹œ ì¬ì‹œë„
                        print(f"    âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
                        self.close_driver()
                        self.driver = None
                        retry_count += 1
                        if retry_count >= max_retries:
                            print(f"    âŒ ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼. ë‹¤ìŒ ê²€ìƒ‰ìœ¼ë¡œ ì´ë™")
                            search_count += 1
                            break
                        print(f"    ğŸ”„ ì¬ì‹œë„ {retry_count}/{max_retries}...")
                        time.sleep(5)

                # ì¬ì‹œë„ ì¹´ìš´í„° ë¦¬ì…‹
                retry_count = 0

        self.results = all_results
        print(f"\nâœ… í¬ë¡¤ë§ ì™„ë£Œ! ì´ {len(all_results)}ê°œ ë§¤ì¥ ì •ë³´ ìˆ˜ì§‘")
        print(f"   ê³ ìœ  URL: {len(visited_urls)}ê°œ")

        # ìµœì¢… ê²°ê³¼ ì €ì¥
        if all_results:
            print(f"\nğŸ“¦ ìµœì¢… ê²°ê³¼ ì €ì¥ ì¤‘...")
            self.save_intermediate_results(all_results, search_count)

        self.close_driver()
        return all_results

    def save_to_csv(self, filename=None):
        """CSV ì €ì¥"""
        if not self.results:
            print("âš ï¸  ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None

        if filename is None:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f'daangn_stores_selenium_{timestamp}.csv'

        output_file = self.output_path / filename

        df = pd.DataFrame(self.results)

        # ì¤‘ë³µ ì œê±°
        df = df.drop_duplicates(subset=['ì „í™”ë²ˆí˜¸', 'ë§í¬'], keep='first')

        # ì •ë ¬
        df = df.sort_values(by=['ì§€ì—­ëª…', 'ë§¤ì¥ëª…'])

        df.to_csv(output_file, index=False, encoding='utf-8-sig')

        print(f"ğŸ’¾ CSV ì €ì¥: {output_file}")
        print(f"   ì´ {len(df)}ê°œ (ì¤‘ë³µ ì œê±° í›„)")

        return output_file

    def upload_to_sheets(self, spreadsheet_url, worksheet_name='ë‹¹ê·¼ë§¤ì¥_Selenium'):
        """Google Sheets ì—…ë¡œë“œ"""
        if not self.results:
            print("âš ï¸  ì—…ë¡œë“œí•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return False

        try:
            print("\nğŸ“¤ Google Sheets ì—…ë¡œë“œ...")

            scopes = [
                'https://www.googleapis.com/auth/spreadsheets',
                'https://www.googleapis.com/auth/drive'
            ]

            creds = Credentials.from_service_account_file(
                str(self.google_api_key_path),
                scopes=scopes
            )

            client = gspread.authorize(creds)

            sheet_id = spreadsheet_url.split('/d/')[1].split('/')[0]
            spreadsheet = client.open_by_key(sheet_id)

            try:
                worksheet = spreadsheet.worksheet(worksheet_name)
            except:
                worksheet = spreadsheet.add_worksheet(title=worksheet_name, rows=3000, cols=10)
                print(f"  âœ… ì›Œí¬ì‹œíŠ¸ '{worksheet_name}' ìƒì„±")

            df = pd.DataFrame(self.results)
            df = df.drop_duplicates(subset=['ì „í™”ë²ˆí˜¸', 'ë§í¬'], keep='first')

            print(f"  ğŸ“Š ì—…ë¡œë“œ: {len(df)}ê°œ")

            # ê¸°ì¡´ ë°ì´í„° í™•ì¸
            existing_data = worksheet.get_all_values()

            # í—¤ë” í¬í•¨ ì‘ì„±
            if len(existing_data) == 0:
                all_data = [df.columns.tolist()] + df.values.tolist()
                batch_size = 100
                worksheet.update('A1', all_data[:batch_size], value_input_option='RAW')
                print(f"  âœ… {min(len(all_data), batch_size)}ê°œ í–‰ ì‘ì„±")

                # ë‚˜ë¨¸ì§€
                remaining = all_data[batch_size:]
                if remaining:
                    time.sleep(60)
                    for i in range(0, len(remaining), batch_size):
                        batch = remaining[i:i+batch_size]
                        worksheet.append_rows(batch, value_input_option='RAW')
                        print(f"  âœ… {len(batch)}ê°œ í–‰ ì¶”ê°€")
                        time.sleep(60)
            else:
                # ì¶”ê°€
                new_rows = df.values.tolist()
                batch_size = 100
                for i in range(0, len(new_rows), batch_size):
                    batch = new_rows[i:i+batch_size]
                    worksheet.append_rows(batch, value_input_option='RAW')
                    print(f"  âœ… {len(batch)}ê°œ í–‰ ì¶”ê°€")
                    if i + batch_size < len(new_rows):
                        time.sleep(60)

            print(f"\nğŸ“Š ì™„ë£Œ: {spreadsheet_url}")
            return True

        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜: {str(e)}")
            import traceback
            traceback.print_exc()
            return False

    def print_summary(self):
        """ê²°ê³¼ ìš”ì•½"""
        if not self.results:
            print("âš ï¸  ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        df = pd.DataFrame(self.results)
        df_unique = df.drop_duplicates(subset=['ì „í™”ë²ˆí˜¸'])

        print("\n" + "=" * 80)
        print("ğŸ“Š ìˆ˜ì§‘ ê²°ê³¼ ìš”ì•½")
        print("=" * 80)
        print(f"ì´ ìˆ˜ì§‘: {len(df)}ê°œ")
        print(f"ê³ ìœ  ì „í™”ë²ˆí˜¸: {len(df_unique)}ê°œ")
        print(f"ê³ ìœ  ì§€ì—­: {df['ì§€ì—­ëª…'].nunique()}ê°œ")

        print("\nğŸ“ ì§€ì—­ë³„ ë§¤ì¥ ìˆ˜:")
        region_counts = df_unique['ì§€ì—­ëª…'].value_counts().head(15)
        for region, count in region_counts.items():
            print(f"   {region}: {count}ê°œ")

        print("\nğŸ“Œ ìƒ˜í”Œ ë°ì´í„°:")
        print(df_unique.head(10).to_string(index=False, max_colwidth=40))


def main():
    """ë©”ì¸ ì‹¤í–‰"""

    SPREADSHEET_URL = "https://docs.google.com/spreadsheets/d/1IDbMaZucrE78gYPK_dhFGFWN_oixcRhlM1sU9tZMJRo/edit?gid=1073623102#gid=1073623102"

    # Headless ëª¨ë“œ í™œì„±í™” (ë¸Œë¼ìš°ì € ì°½ ìˆ¨ê¹€, ì•ˆì •ì„± í–¥ìƒ)
    crawler = DaangnStoreCrawlerSelenium(headless=True)

    # í¬ë¡¤ë§ ì„¤ì •
    # max_searches ê°’ ì¡°ì •:
    # - í…ŒìŠ¤íŠ¸: max_searches=10
    # - ì†Œê·œëª¨: max_searches=50 (ì„œìš¸ ì¼ë¶€ êµ¬ + ì£¼ìš” í‚¤ì›Œë“œ)
    # - ì¤‘ê·œëª¨: max_searches=200 (ì„œìš¸ ì „ì²´ + ê²½ê¸° ì¼ë¶€)
    # - ëŒ€ê·œëª¨: max_searches=500 (ì„œìš¸ + ìˆ˜ë„ê¶Œ ì „ì²´)
    # - ì „ì²´: max_searches=2000 (ëª¨ë“  ì§€ì—­ Ã— ëª¨ë“  í‚¤ì›Œë“œ)

    # save_interval: ì¤‘ê°„ ì €ì¥ ê°„ê²© (ê¸°ë³¸ 50ê°œë§ˆë‹¤)

    print("\nğŸ¯ ì„œìš¸/ìˆ˜ë„ê¶Œ ì§‘ì¤‘ í¬ë¡¤ë§ ëª¨ë“œ")
    print(f"ğŸ“ ì´ ì§€ì—­ ìˆ˜: {len(crawler.regions)}ê°œ")
    print(f"ğŸ”‘ ì´ í‚¤ì›Œë“œ ìˆ˜: {len(crawler.keywords)}ê°œ")
    print(f"ğŸ“Š ìµœëŒ€ ê²€ìƒ‰ ì¡°í•©: {len(crawler.regions) * len(crawler.keywords)}ê°œ")
    print("=" * 80)

    # í¬ë¡¤ë§ ì‹¤í–‰ (ì „ì²´ ì§€ì—­ Ã— ì „ì²´ í‚¤ì›Œë“œ)
    # 2000ê°œ ê²€ìƒ‰ = 79ê°œ ì§€ì—­ Ã— 26ê°œ í‚¤ì›Œë“œ = 2,054ê°œ ì¡°í•©
    # save_interval=50: 50ê°œ ê²€ìƒ‰ë§ˆë‹¤ ì¤‘ê°„ ì €ì¥
    results = crawler.crawl(max_searches=2000, save_interval=50)

    # ìš”ì•½
    crawler.print_summary()

    # CSV ì €ì¥
    crawler.save_to_csv()

    # Google Sheets ì—…ë¡œë“œ (daangn ì‹œíŠ¸ì— ì—…ë¡œë“œ)
    if results:
        print("\nâ³ Google Sheets ì—…ë¡œë“œ ì¤‘...")
        crawler.upload_to_sheets(SPREADSHEET_URL, worksheet_name='daangn')

    print("\n" + "=" * 80)
    print("âœ… ì™„ë£Œ!")
    print("=" * 80)


if __name__ == "__main__":
    main()
