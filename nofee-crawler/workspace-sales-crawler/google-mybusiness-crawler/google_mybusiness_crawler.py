"""
êµ¬ê¸€ ë§ˆì´ë¹„ì¦ˆë‹ˆìŠ¤ íœ´ëŒ€í° ë§¤ì¥ í¬ë¡¤ëŸ¬ (Selenium ë²„ì „)
- Chrome ë¸Œë¼ìš°ì €ë¥¼ ì§ì ‘ ì œì–´
- Google Maps/MyBusinessì—ì„œ ë§¤ì¥ ê²€ìƒ‰
- 010 ì „í™”ë²ˆí˜¸ë§Œ í•„í„°ë§í•˜ì—¬ ìˆ˜ì§‘
"""

import time
import re
import random
from datetime import datetime
from pathlib import Path
import pandas as pd
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from selenium.common.exceptions import TimeoutException, NoSuchElementException

class GoogleMyBusinessCrawler:
    """êµ¬ê¸€ ë§ˆì´ë¹„ì¦ˆë‹ˆìŠ¤ íœ´ëŒ€í° ë§¤ì¥ í¬ë¡¤ëŸ¬"""

    def __init__(self, google_api_key_path=None, headless=False):
        self.base_path = Path(__file__).parent
        self.output_path = self.base_path / 'output'
        self.output_path.mkdir(exist_ok=True)

        if google_api_key_path is None:
            self.google_api_key_path = Path('/Users/jacob/Desktop/dev/config/google_api_key.json')
        else:
            self.google_api_key_path = Path(google_api_key_path)

        # Chrome ì˜µì…˜ ì„¤ì •
        self.chrome_options = Options()
        if headless:
            self.chrome_options.add_argument('--headless')
        self.chrome_options.add_argument('--no-sandbox')
        self.chrome_options.add_argument('--disable-dev-shm-usage')
        self.chrome_options.add_argument('--disable-blink-features=AutomationControlled')

        # User-Agent ëœë¤í™”
        user_agents = [
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        ]
        self.chrome_options.add_argument(f'user-agent={random.choice(user_agents)}')
        self.chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
        self.chrome_options.add_experimental_option('useAutomationExtension', False)
        self.chrome_options.add_argument('--window-size=1920,1080')

        self.driver = None

        # ì„œìš¸/ìˆ˜ë„ê¶Œ ì§€ì—­
        self.regions = [
            'ì„œìš¸ ê°•ë‚¨êµ¬', 'ì„œìš¸ ê°•ë™êµ¬', 'ì„œìš¸ ê°•ë¶êµ¬', 'ì„œìš¸ ê°•ì„œêµ¬',
            'ì„œìš¸ ê´€ì•…êµ¬', 'ì„œìš¸ ê´‘ì§„êµ¬', 'ì„œìš¸ êµ¬ë¡œêµ¬', 'ì„œìš¸ ê¸ˆì²œêµ¬',
            'ì„œìš¸ ë…¸ì›êµ¬', 'ì„œìš¸ ë„ë´‰êµ¬', 'ì„œìš¸ ë™ëŒ€ë¬¸êµ¬', 'ì„œìš¸ ë™ì‘êµ¬',
            'ì„œìš¸ ë§ˆí¬êµ¬', 'ì„œìš¸ ì„œëŒ€ë¬¸êµ¬', 'ì„œìš¸ ì„œì´ˆêµ¬', 'ì„œìš¸ ì„±ë™êµ¬',
            'ì„œìš¸ ì„±ë¶êµ¬', 'ì„œìš¸ ì†¡íŒŒêµ¬', 'ì„œìš¸ ì–‘ì²œêµ¬', 'ì„œìš¸ ì˜ë“±í¬êµ¬',
            'ì„œìš¸ ìš©ì‚°êµ¬', 'ì„œìš¸ ì€í‰êµ¬', 'ì„œìš¸ ì¢…ë¡œêµ¬', 'ì„œìš¸ ì¤‘êµ¬', 'ì„œìš¸ ì¤‘ë‘êµ¬',
            'ê²½ê¸° ì„±ë‚¨', 'ê²½ê¸° ìˆ˜ì›', 'ê²½ê¸° ì•ˆì–‘', 'ê²½ê¸° ì•ˆì‚°', 'ê²½ê¸° ìš©ì¸',
            'ê²½ê¸° ê´‘ëª…', 'ê²½ê¸° ê³¼ì²œ', 'ê²½ê¸° ì˜ì™•', 'ê²½ê¸° êµ°í¬', 'ê²½ê¸° ì‹œí¥',
            'ê²½ê¸° ë¶€ì²œ', 'ê²½ê¸° ê´‘ì£¼', 'ê²½ê¸° í•˜ë‚¨', 'ê²½ê¸° í™”ì„±', 'ê²½ê¸° ì˜¤ì‚°',
            'ê²½ê¸° ê³ ì–‘', 'ê²½ê¸° íŒŒì£¼', 'ê²½ê¸° ì˜ì •ë¶€', 'ê²½ê¸° ì–‘ì£¼', 'ê²½ê¸° ë™ë‘ì²œ',
            'ê²½ê¸° ë‚¨ì–‘ì£¼', 'ê²½ê¸° êµ¬ë¦¬', 'ê²½ê¸° í¬ì²œ', 'ê²½ê¸° ì—°ì²œ', 'ê²½ê¸° ê°€í‰',
            'ê²½ê¸° ì´ì²œ', 'ê²½ê¸° ì—¬ì£¼', 'ê²½ê¸° ì–‘í‰', 'ê²½ê¸° ê¹€í¬',
            'ì¸ì²œ ì¤‘êµ¬', 'ì¸ì²œ ë™êµ¬', 'ì¸ì²œ ë‚¨êµ¬', 'ì¸ì²œ ì—°ìˆ˜êµ¬', 'ì¸ì²œ ë‚¨ë™êµ¬',
            'ì¸ì²œ ë¶€í‰êµ¬', 'ì¸ì²œ ê³„ì–‘êµ¬', 'ì¸ì²œ ì„œêµ¬', 'ì¸ì²œ ê°•í™”êµ°', 'ì¸ì²œ ì˜¹ì§„êµ°',
        ]

        # ê²€ìƒ‰ í‚¤ì›Œë“œ
        self.keywords = [
            'íœ´ëŒ€í°ë§¤ì¥', 'íœ´ëŒ€í°ì„±ì§€', 'ìŠ¤ë§ˆíŠ¸í°ë§¤ì¥', 'í°ë§¤ì¥',
            'íœ´ëŒ€í°ê°€ê²Œ', 'í•¸ë“œí°ê°€ê²Œ', 'ë™ë„¤íœ´ëŒ€í°ë§¤ì¥',
            'íœ´ëŒ€í°íŒë§¤', 'íœ´ëŒ€í°ëŒ€ë¦¬ì ', 'í•¸ë“œí°ë§¤ì¥', 'í•¸ë“œí°íŒë§¤',
            'ìŠ¤ë§ˆíŠ¸í°íŒë§¤', 'íœ´ëŒ€í°ê°œí†µ', 'ê¸°ê¸°ë³€ê²½', 'ë²ˆí˜¸ì´ë™',
            'ì•„ì´í°', 'ê°¤ëŸ­ì‹œ', 'ì•„ì´í°ë§¤ì¥', 'ê°¤ëŸ­ì‹œë§¤ì¥',
            'ì•„ì´í°íŒë§¤', 'ê°¤ëŸ­ì‹œíŒë§¤',
            'íœ´ëŒ€í°ë§¤ì¥ì¶”ì²œ', 'ë¯¿ì„ë§Œí•œíœ´ëŒ€í°ë§¤ì¥', 'ì•ˆì „í•œê°œí†µ',
            'íœ´ëŒ€í°ì„±ì§€í›„ê¸°', 'íœ´ëŒ€í°ë§¤ì¥í›„ê¸°',
        ]

        self.collected_stores = set()

    def init_driver(self):
        """Chrome ë“œë¼ì´ë²„ ì´ˆê¸°í™”"""
        try:
            self.driver = webdriver.Chrome(options=self.chrome_options)
            self.driver.execute_cdp_cmd('Page.addScriptToEvaluateOnNewDocument', {
                'source': '''
                    Object.defineProperty(navigator, 'webdriver', {
                        get: () => undefined
                    });
                '''
            })
            print("âœ… Chrome ë¸Œë¼ìš°ì € ì‹œì‘ ì™„ë£Œ")
            return True
        except Exception as e:
            print(f"âŒ Chrome ë“œë¼ì´ë²„ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
            return False

    def close_driver(self):
        """Chrome ë“œë¼ì´ë²„ ì¢…ë£Œ"""
        if self.driver:
            try:
                self.driver.quit()
                self.driver = None
            except:
                pass

    def extract_010_phones(self, text):
        """í…ìŠ¤íŠ¸ì—ì„œ 010 ì „í™”ë²ˆí˜¸ë§Œ ì¶”ì¶œ"""
        if not text:
            return []

        pattern = r'010[-\s]?\d{3,4}[-\s]?\d{4}'
        phones = re.findall(pattern, text)

        normalized = []
        for phone in phones:
            digits = re.sub(r'[-\s]', '', phone)
            if len(digits) == 11:
                formatted = f"{digits[:3]}-{digits[3:7]}-{digits[7:]}"
                normalized.append(formatted)
            elif len(digits) == 10:
                formatted = f"{digits[:3]}-{digits[3:6]}-{digits[6:]}"
                normalized.append(formatted)

        return list(set(normalized))

    def search_google_maps(self, region, keyword):
        """êµ¬ê¸€ ë§µìŠ¤ì—ì„œ ê²€ìƒ‰"""
        query = f"{region} {keyword}"
        search_url = f"https://www.google.com/maps/search/{query}"

        try:
            self.driver.get(search_url)
            time.sleep(random.uniform(4, 6))

            stores = []

            # êµ¬ê¸€ ë§µìŠ¤ ê²€ìƒ‰ ê²°ê³¼ ì°¾ê¸°
            try:
                # ìŠ¤í¬ë¡¤ ê°€ëŠ¥í•œ ì‚¬ì´ë“œë°” ì˜ì—­
                scrollable_div = self.driver.find_element(By.CSS_SELECTOR, "div[role='feed']")

                # ìŠ¤í¬ë¡¤í•˜ì—¬ ë” ë§ì€ ê²°ê³¼ ë¡œë“œ
                for _ in range(3):  # 3ë²ˆ ìŠ¤í¬ë¡¤
                    self.driver.execute_script("arguments[0].scrollTop = arguments[0].scrollHeight", scrollable_div)
                    time.sleep(1)

                # ë§¤ì¥ ì•„ì´í…œ ì°¾ê¸°
                items = self.driver.find_elements(By.CSS_SELECTOR, "div[role='article']")

                print(f"    ğŸ“Œ {len(items)}ê°œ ê²€ìƒ‰ ê²°ê³¼ ë°œê²¬")

                for item in items[:20]:  # ìƒìœ„ 20ê°œë§Œ
                    try:
                        # ë§¤ì¥ëª…
                        name_elem = item.find_element(By.CSS_SELECTOR, "div.qBF1Pd")
                        store_name = name_elem.text.strip()

                        # ë§í¬ (í´ë¦­í•˜ì—¬ ìƒì„¸ í˜ì´ì§€ë¡œ ì´ë™)
                        link_elem = item.find_element(By.CSS_SELECTOR, "a")
                        link = link_elem.get_attribute('href')

                        stores.append({
                            'name': store_name,
                            'link': link,
                            'region': region
                        })

                    except Exception as e:
                        continue

            except Exception as e:
                print(f"    âš ï¸  ê²€ìƒ‰ ê²°ê³¼ íŒŒì‹± ì‹¤íŒ¨: {str(e)}")

            return stores

        except Exception as e:
            print(f"    âŒ ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
            return []

    def get_store_detail(self, store):
        """ë§¤ì¥ ìƒì„¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°"""
        try:
            self.driver.get(store['link'])
            time.sleep(random.uniform(3, 4))

            # í˜ì´ì§€ ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ 010 ë²ˆí˜¸ ì°¾ê¸°
            page_text = self.driver.find_element(By.TAG_NAME, 'body').text
            phones = self.extract_010_phones(page_text)

            if phones:
                return {
                    'region': store['region'],
                    'name': store['name'],
                    'phones': phones,
                    'link': store['link']
                }

            return None

        except Exception as e:
            return None

    def save_intermediate_results(self, results, search_count):
        """ì¤‘ê°„ ê²°ê³¼ ì €ì¥"""
        if not results:
            return

        try:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = self.output_path / f'google_stores_intermediate_{search_count}searches_{timestamp}.csv'

            df = pd.DataFrame(results)
            df.to_csv(filename, index=False, encoding='utf-8-sig')
            print(f"    ğŸ’¾ ì¤‘ê°„ ì €ì¥ ì™„ë£Œ: {len(results)}ê°œ ë§¤ì¥ â†’ {filename.name}")
        except Exception as e:
            print(f"    âš ï¸  ì¤‘ê°„ ì €ì¥ ì‹¤íŒ¨: {str(e)}")

    def crawl(self, max_searches=2000, save_interval=50):
        """êµ¬ê¸€ ë§ˆì´ë¹„ì¦ˆë‹ˆìŠ¤ í¬ë¡¤ë§ ì‹¤í–‰"""
        print("=" * 80)
        print("ğŸ—ºï¸  êµ¬ê¸€ ë§ˆì´ë¹„ì¦ˆë‹ˆìŠ¤ íœ´ëŒ€í° ë§¤ì¥ í¬ë¡¤ëŸ¬")
        print("=" * 80)
        print(f"ğŸ“ ì´ ì§€ì—­ ìˆ˜: {len(self.regions)}ê°œ")
        print(f"ğŸ”‘ ì´ í‚¤ì›Œë“œ ìˆ˜: {len(self.keywords)}ê°œ")
        print("=" * 80)

        all_results = []
        search_count = 0
        max_retries = 3
        retry_count = 0

        for region in self.regions:
            for keyword in self.keywords:
                search_count += 1

                if search_count > max_searches:
                    break

                print(f"\n[{search_count}/{max_searches}] ğŸ” {region} {keyword}")

                while retry_count < max_retries:
                    try:
                        if self.driver is None:
                            print("    ğŸ”„ ë“œë¼ì´ë²„ ì¬ì´ˆê¸°í™” ì¤‘...")
                            if not self.init_driver():
                                retry_count += 1
                                time.sleep(5)
                                continue
                            retry_count = 0

                        stores = self.search_google_maps(region, keyword)

                        for store in stores:
                            detail = self.get_store_detail(store)
                            if detail:
                                for phone in detail['phones']:
                                    store_key = f"{detail['name']}_{phone}"
                                    if store_key not in self.collected_stores:
                                        self.collected_stores.add(store_key)
                                        all_results.append({
                                            'ì§€ì—­ëª…': detail['region'],
                                            'ë§¤ì¥ëª…': detail['name'],
                                            'ì „í™”ë²ˆí˜¸': phone,
                                            'ë§í¬': detail['link']
                                        })
                                        print(f"      ğŸ’¾ ì €ì¥: {detail['name']} ({phone})")

                            time.sleep(random.uniform(1, 2))

                        if search_count % save_interval == 0:
                            print(f"\nğŸ“¦ ì¤‘ê°„ ì €ì¥ ì‹œì  ({search_count}ë²ˆ ê²€ìƒ‰ ì™„ë£Œ)")
                            self.save_intermediate_results(all_results, search_count)

                        # Googleì€ ì¢€ ë” ê¸´ ëŒ€ê¸° ì‹œê°„
                        wait_time = random.uniform(10, 15)
                        print(f"    â³ {wait_time:.1f}ì´ˆ ëŒ€ê¸° ì¤‘... (Google ë´‡ íƒì§€ íšŒí”¼)")
                        time.sleep(wait_time)

                        break

                    except Exception as e:
                        print(f"    âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
                        self.close_driver()
                        self.driver = None
                        retry_count += 1
                        if retry_count >= max_retries:
                            print(f"    âŒ ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼")
                            break
                        time.sleep(5)

                retry_count = 0

            if search_count > max_searches:
                break

        print("\n" + "=" * 80)
        print("âœ… ìµœì¢… í¬ë¡¤ë§ ì™„ë£Œ")
        print("=" * 80)
        print(f"ğŸ“Š ì´ ê²€ìƒ‰ íšŸìˆ˜: {search_count}íšŒ")
        print(f"ğŸ’¾ ìˆ˜ì§‘ëœ ë§¤ì¥: {len(all_results)}ê°œ")

        if all_results:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            df = pd.DataFrame(all_results)
            final_filename = self.output_path / f'google_stores_{timestamp}.csv'
            df.to_csv(final_filename, index=False, encoding='utf-8-sig')
            print(f"ğŸ’¾ ìµœì¢… íŒŒì¼ ì €ì¥: {final_filename}")

            self.save_intermediate_results(all_results, search_count)

        self.close_driver()
        return all_results

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ—ºï¸  êµ¬ê¸€ ë§ˆì´ë¹„ì¦ˆë‹ˆìŠ¤ í¬ë¡¤ë§ ì‹œì‘...")

    crawler = GoogleMyBusinessCrawler(headless=True)
    results = crawler.crawl(max_searches=2000, save_interval=50)

    print(f"\nğŸ‰ í¬ë¡¤ë§ ì™„ë£Œ! ì´ {len(results)}ê°œ ë§¤ì¥ ìˆ˜ì§‘")

if __name__ == "__main__":
    main()
