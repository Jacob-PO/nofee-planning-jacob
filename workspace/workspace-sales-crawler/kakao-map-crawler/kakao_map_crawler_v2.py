"""
ì¹´ì¹´ì˜¤ë§µ íœ´ëŒ€í° ì„±ì§€ í¬ë¡¤ëŸ¬ (ê°œì„  ë²„ì „)
- íŠ¹ì • ê²€ìƒ‰ URLì—ì„œ ë§¤ì¥ ì •ë³´ ìˆ˜ì§‘
- Seleniumì„ ì‚¬ìš©í•œ ë™ì  ì½˜í…ì¸  í¬ë¡¤ë§
- CSV íŒŒì¼ë¡œ ê²°ê³¼ ì €ì¥
"""

import time
import re
import random
from datetime import datetime
from pathlib import Path
from urllib.parse import quote_plus
import pandas as pd
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from selenium.common.exceptions import TimeoutException, NoSuchElementException, StaleElementReferenceException

class KakaoMapCrawlerV2:
    """ì¹´ì¹´ì˜¤ë§µ íœ´ëŒ€í° ë§¤ì¥ í¬ë¡¤ëŸ¬ V2"""

    def __init__(self, headless=False):
        self.base_path = Path(__file__).parent
        self.output_path = self.base_path / 'output'
        self.output_path.mkdir(exist_ok=True)

        # Chrome ì˜µì…˜ ì„¤ì •
        self.chrome_options = Options()
        if headless:
            self.chrome_options.add_argument('--headless')
        self.chrome_options.add_argument('--no-sandbox')
        self.chrome_options.add_argument('--disable-dev-shm-usage')
        self.chrome_options.add_argument('--disable-blink-features=AutomationControlled')
        self.chrome_options.add_argument('--window-size=1920,1080')
        
        # User-Agent ì„¤ì •
        user_agent = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        self.chrome_options.add_argument(f'user-agent={user_agent}')
        self.chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
        self.chrome_options.add_experimental_option('useAutomationExtension', False)

        self.driver = None
        self.collected_stores = set()

    def init_driver(self):
        """Chrome ë“œë¼ì´ë²„ ì´ˆê¸°í™”"""
        try:
            self.driver = webdriver.Chrome(options=self.chrome_options)
            self.driver.execute_cdp_cmd('Page.addScriptToEvaluateOnNewDocument', {
                'source': '''
                    Object.defineProperty(navigator, 'webdriver', {
                        get: () => undefined
                    });
                '''
            })
            print("âœ… Chrome ë¸Œë¼ìš°ì € ì‹œì‘ ì™„ë£Œ")
            return True
        except Exception as e:
            print(f"âŒ Chrome ë“œë¼ì´ë²„ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
            return False

    def close_driver(self):
        """Chrome ë“œë¼ì´ë²„ ì¢…ë£Œ"""
        if self.driver:
            try:
                self.driver.quit()
                self.driver = None
            except:
                pass

    def extract_phone_numbers(self, text):
        """í…ìŠ¤íŠ¸ì—ì„œ ì „í™”ë²ˆí˜¸ ì¶”ì¶œ (010, 02, 031 ë“± ëª¨ë‘ í¬í•¨)"""
        if not text:
            return []

        # ë‹¤ì–‘í•œ ì „í™”ë²ˆí˜¸ íŒ¨í„´
        patterns = [
            r'010[-\s]?\d{3,4}[-\s]?\d{4}',  # 010
            r'02[-\s]?\d{3,4}[-\s]?\d{4}',   # ì„œìš¸
            r'0\d{2}[-\s]?\d{3,4}[-\s]?\d{4}' # ê¸°íƒ€ ì§€ì—­ë²ˆí˜¸
        ]
        
        phones = []
        for pattern in patterns:
            matches = re.findall(pattern, text)
            phones.extend(matches)

        # ì „í™”ë²ˆí˜¸ ì •ê·œí™”
        normalized = []
        for phone in phones:
            digits = re.sub(r'[-\s]', '', phone)
            if len(digits) >= 9 and len(digits) <= 11:
                if digits.startswith('02'):
                    if len(digits) == 9:
                        formatted = f"{digits[:2]}-{digits[2:5]}-{digits[5:]}"
                    else:
                        formatted = f"{digits[:2]}-{digits[2:6]}-{digits[6:]}"
                elif len(digits) == 10:
                    formatted = f"{digits[:3]}-{digits[3:6]}-{digits[6:]}"
                elif len(digits) == 11:
                    formatted = f"{digits[:3]}-{digits[3:7]}-{digits[7:]}"
                else:
                    formatted = phone
                normalized.append(formatted)

        unique_numbers = []
        seen = set()
        for num in normalized:
            if num not in seen:
                seen.add(num)
                unique_numbers.append(num)
        return unique_numbers

    def extract_mobile_number(self, text):
        """í…ìŠ¤íŠ¸ì—ì„œ 010ìœ¼ë¡œ ì‹œì‘í•˜ëŠ” ë²ˆí˜¸ë§Œ ì¶”ì¶œ"""
        numbers = self.extract_phone_numbers(text)
        for number in numbers:
            if number.startswith('010'):
                return number
        return ""

    def is_mobile_number(self, phone):
        return bool(phone and phone.startswith('010'))

    def scroll_page(self, scroll_container_selector):
        """í˜ì´ì§€ ìŠ¤í¬ë¡¤í•˜ì—¬ ëª¨ë“  ê²°ê³¼ ë¡œë“œ"""
        try:
            scroll_container = self.driver.find_element(By.CSS_SELECTOR, scroll_container_selector)
            last_height = self.driver.execute_script("return arguments[0].scrollHeight", scroll_container)
            
            scroll_count = 0
            max_scrolls = 20  # ë” ë§ì´ ìŠ¤í¬ë¡¤
            
            while scroll_count < max_scrolls:
                # ìŠ¤í¬ë¡¤ ë‹¤ìš´
                self.driver.execute_script("arguments[0].scrollTo(0, arguments[0].scrollHeight);", scroll_container)
                time.sleep(random.uniform(1.5, 2.5))
                
                # ìƒˆë¡œìš´ ë†’ì´ í™•ì¸
                new_height = self.driver.execute_script("return arguments[0].scrollHeight", scroll_container)
                
                if new_height == last_height:
                    break
                    
                last_height = new_height
                scroll_count += 1
                print(f"    ğŸ“œ ìŠ¤í¬ë¡¤ {scroll_count}íšŒ ì™„ë£Œ")
                
        except Exception as e:
            print(f"    âš ï¸  ìŠ¤í¬ë¡¤ ì¤‘ ì˜¤ë¥˜: {str(e)}")

    def click_next_page(self):
        """
        ë‹¤ìŒ í˜ì´ì§€ë¡œ ì´ë™.
        - í˜„ì¬ í˜ì´ì§€ ë¸”ë¡(1~5, 6~10 ë“±)ì—ì„œ ë‹¤ìŒ ìˆ«ì ë²„íŠ¼ì„ ìš°ì„  í´ë¦­
        - ë¸”ë¡ ëì— ë„ë‹¬í•˜ë©´ 'ë‹¤ìŒ' ë²„íŠ¼ì„ ëˆŒëŸ¬ ë‹¤ìŒ ë¸”ë¡ìœ¼ë¡œ ì „í™˜
        """
        try:
            pagination = self.driver.find_element(By.CSS_SELECTOR, "#info\\.search\\.page .pageWrap")
            page_links = pagination.find_elements(By.CSS_SELECTOR, "a[id^='info\\.search\\.page\\.no']")
            
            found_active = False
            for link in page_links:
                classes_raw = (link.get_attribute("class") or "").upper()
                class_tokens = classes_raw.replace(",", " ").split()
                if "HIDDEN" in class_tokens:
                    continue
                if "ACTIVE" in class_tokens:
                    found_active = True
                    continue
                if found_active:
                    self.driver.execute_script("arguments[0].click();", link)
                    time.sleep(random.uniform(2, 3))
                    return True
            
            # í˜„ì¬ ë¸”ë¡ì˜ ë§ˆì§€ë§‰ í˜ì´ì§€ê¹Œì§€ ë³¸ ê²½ìš° - ë‹¤ìŒ ë¸”ë¡ìœ¼ë¡œ ì „í™˜
            next_button = pagination.find_element(By.CSS_SELECTOR, "#info\\.search\\.page\\.next")
            button_class = (next_button.get_attribute("class") or "").lower()
            if "disabled" in button_class:
                return False
            
            self.driver.execute_script("arguments[0].click();", next_button)
            time.sleep(random.uniform(2, 3))
            return True
            
        except NoSuchElementException:
            return False
        except Exception as e:
            print(f"    âš ï¸  ë‹¤ìŒ í˜ì´ì§€ ì´ë™ ì‹¤íŒ¨: {str(e)}")
            return False

    def get_current_page_number(self):
        """í˜„ì¬ í˜ì´ì§€ ë²ˆí˜¸ í…ìŠ¤íŠ¸ ë°˜í™˜"""
        try:
            active = self.driver.find_element(By.CSS_SELECTOR, "#info\\.search\\.page .ACTIVE")
            return active.text.strip()
        except Exception:
            return ""

    def get_store_info_from_list(self):
        """ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ì—ì„œ ë§¤ì¥ ì •ë³´ ì¶”ì¶œ (í˜„ì¬ í˜ì´ì§€ë§Œ)"""
        stores = []
        
        try:
            # ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ ëŒ€ê¸°
            wait = WebDriverWait(self.driver, 10)
            wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, "#info\\.search\\.place\\.list")))
            
            # ìŠ¤í¬ë¡¤í•˜ì—¬ ëª¨ë“  ê²°ê³¼ ë¡œë“œ
            print("    ğŸ“œ ê²€ìƒ‰ ê²°ê³¼ ìŠ¤í¬ë¡¤ ì¤‘...")
            self.scroll_page("#info\\.search\\.place\\.list")
            
            time.sleep(2)
            
            # ëª¨ë“  ë§¤ì¥ í•­ëª© ê°€ì ¸ì˜¤ê¸°
            items = self.driver.find_elements(By.CSS_SELECTOR, "#info\\.search\\.place\\.list > li")
            print(f"    ğŸ“Œ í˜„ì¬ í˜ì´ì§€: {len(items)}ê°œ ë§¤ì¥ ë°œê²¬")
            
            for idx, item in enumerate(items, 1):
                try:
                    # ë§¤ì¥ëª… - ì •í™•í•œ ì„ íƒì ì‚¬ìš©
                    name_elem = item.find_element(By.CSS_SELECTOR, ".head_item .tit_name .link_name")
                    store_name = name_elem.text.strip()
                    
                    # ì¹´ì¹´ì˜¤ë§µ ë§í¬
                    link = name_elem.get_attribute('href')
                    
                    # ì£¼ì†Œ ì •ë³´ - XPath ê¸°ë°˜ìœ¼ë¡œ ì •í™•í•˜ê²Œ ì¶”ì¶œ
                    address = ""
                    try:
                        addr_elem = item.find_element(By.CSS_SELECTOR, ".info_item .addr p")
                        address = addr_elem.text.strip()
                    except:
                        pass
                    
                    # ì „í™”ë²ˆí˜¸ - ë‹¤ì–‘í•œ ì„ íƒì + í…ìŠ¤íŠ¸ ì¶”ì¶œ
                    phone_text = ""
                    phone_selectors = [
                        ".info_item .tel",
                        ".info_item .contact .phone",
                        ".contact [data-id='phone']",
                        "[data-id='phone'].phone",
                    ]
                    for selector in phone_selectors:
                        if phone_text:
                            break
                        try:
                            phone_elem = item.find_element(By.CSS_SELECTOR, selector)
                            candidate = phone_elem.text.strip()
                            if candidate:
                                phone_text = candidate
                        except NoSuchElementException:
                            continue
                        except Exception:
                            continue
                    
                    phone = self.extract_mobile_number(phone_text)
                    if not phone:
                        # ì¹´ë“œ ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ 010 ë²ˆí˜¸ ë‹¤ì‹œ í™•ì¸
                        phone = self.extract_mobile_number(item.text)
                    
                    # ì¹´í…Œê³ ë¦¬
                    category = ""
                    try:
                        cat_elem = item.find_element(By.CSS_SELECTOR, ".head_item .subcategory")
                        category = cat_elem.text.strip()
                    except:
                        pass
                    
                    store_data = {
                        'name': store_name,
                        'address': address,
                        'phone': phone,
                        'category': category,
                        'link': link
                    }
                    
                    stores.append(store_data)
                    phone_log = phone if phone else '010ì—†ìŒ'
                    print(f"    [{idx}] {store_name} - {address[:20] if address else 'ì£¼ì†Œì—†ìŒ'} - {phone_log}")
                    
                except StaleElementReferenceException:
                    print(f"    âš ï¸  í•­ëª© {idx} ìŠ¤í‚µ (ìš”ì†Œ ë³€ê²½ë¨)")
                    continue
                except Exception as e:
                    print(f"    âš ï¸  í•­ëª© {idx} íŒŒì‹± ì‹¤íŒ¨: {str(e)}")
                    continue
                    
        except TimeoutException:
            print("    âŒ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        except Exception as e:
            print(f"    âŒ ë§¤ì¥ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
            
        return stores

    def get_detailed_info(self, store):
        """ë§¤ì¥ ìƒì„¸ í˜ì´ì§€ì—ì„œ ì¶”ê°€ ì •ë³´ ìˆ˜ì§‘ - ì‚¬ìš© ì•ˆ í•¨"""
        # ë¦¬ìŠ¤íŠ¸ì—ì„œ ì´ë¯¸ ëª¨ë“  ì •ë³´ë¥¼ ìˆ˜ì§‘í•˜ë¯€ë¡œ ìƒì„¸ í˜ì´ì§€ ë°©ë¬¸ ë¶ˆí•„ìš”
        return store

    def crawl_from_url(self, url, get_details=True, save_to_file=True, keyword=None):
        """íŠ¹ì • URLì—ì„œ í¬ë¡¤ë§"""
        print("=" * 80)
        print("ğŸ—ºï¸  ì¹´ì¹´ì˜¤ë§µ íœ´ëŒ€í° ë§¤ì¥ í¬ë¡¤ëŸ¬ V2")
        print("=" * 80)
        print(f"ğŸ”— URL: {url}")
        print("=" * 80)
        if keyword:
            print(f"ğŸ”‘ ê²€ìƒ‰ í‚¤ì›Œë“œ: {keyword}")
            print("=" * 80)
        
        if not self.init_driver():
            return []
        
        all_results = []
        
        try:
            # ì¹´ì¹´ì˜¤ë§µ í˜ì´ì§€ ì—´ê¸°
            print("\nğŸ“± ì¹´ì¹´ì˜¤ë§µ í˜ì´ì§€ ë¡œë”© ì¤‘...")
            self.driver.get(url)
            time.sleep(random.uniform(4, 6))
            
            # í˜ì´ì§€ë„¤ì´ì…˜ì„ í†µí•´ ëª¨ë“  ë§¤ì¥ ì •ë³´ ìˆ˜ì§‘
            print("\nğŸ” ë§¤ì¥ ì •ë³´ ìˆ˜ì§‘ ì¤‘ (í˜ì´ì§€ë„¤ì´ì…˜ ì²˜ë¦¬)...")
            all_stores = []
            processed_pages = 0
            
            while True:
                current_page_label = self.get_current_page_number()
                display_page = current_page_label or str(processed_pages + 1)
                print(f"\n[í˜ì´ì§€ {display_page}] ìˆ˜ì§‘ ì¤‘...")
                stores = self.get_store_info_from_list()
                
                if not stores:
                    print(f"    âš ï¸  í˜ì´ì§€ {display_page}ì—ì„œ ë§¤ì¥ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì¢…ë£Œí•©ë‹ˆë‹¤.")
                    break
                
                all_stores.extend(stores)
                print(f"    âœ… í˜ì´ì§€ {display_page}: {len(stores)}ê°œ ìˆ˜ì§‘ (ëˆ„ì : {len(all_stores)}ê°œ)")
                
                processed_pages += 1
                # ë‹¤ìŒ í˜ì´ì§€ë¡œ ì´ë™
                if not self.click_next_page():
                    print(f"\n    âœ… ë§ˆì§€ë§‰ í˜ì´ì§€ ë„ë‹¬. ì´ {processed_pages}í˜ì´ì§€ ìˆ˜ì§‘ ì™„ë£Œ")
                    break
                
                time.sleep(random.uniform(1, 2))
            
            stores = all_stores
            print(f"\nğŸ“Š 1ì°¨ ìˆ˜ì§‘ ì™„ë£Œ: {len(stores)}ê°œ ë§¤ì¥ (ì´ {processed_pages}í˜ì´ì§€)")
            
            # ìˆ˜ì§‘ëœ ë§¤ì¥ ì •ë³´ë¥¼ ê²°ê³¼ì— ì¶”ê°€
            print(f"\nğŸ“ ê²°ê³¼ ì •ë¦¬ ì¤‘...")
            for store in stores:
                phone = store.get('phone', '')
                if not self.is_mobile_number(phone):
                    continue
                store_key = f"{store['name']}_{phone}"
                if store_key not in self.collected_stores:
                    self.collected_stores.add(store_key)
                    all_results.append({
                        'ë§¤ì¥ëª…': store['name'],
                        'ì£¼ì†Œ': store.get('address', ''),
                        'ì „í™”ë²ˆí˜¸': phone,
                        'ì¹´í…Œê³ ë¦¬': store.get('category', ''),
                        'ì¹´ì¹´ì˜¤ë§µë§í¬': store.get('link', '')
                    })
            
            print("\n" + "=" * 80)
            print("âœ… í¬ë¡¤ë§ ì™„ë£Œ")
            print("=" * 80)
            print(f"ğŸ’¾ ìˆ˜ì§‘ëœ ë§¤ì¥: {len(all_results)}ê°œ")
            
            # CSV íŒŒì¼ë¡œ ì €ì¥
            if save_to_file and all_results:
                self.save_results(all_results)
                
        except Exception as e:
            print(f"\nâŒ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            import traceback
            traceback.print_exc()
        finally:
            self.close_driver()
        
        return all_results

    def crawl_keywords(self, keywords, get_details=True):
        """ì—¬ëŸ¬ í‚¤ì›Œë“œë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ê²€ìƒ‰"""
        aggregated_results = []
        for keyword in keywords:
            url = self.build_search_url(keyword)
            keyword_results = self.crawl_from_url(
                url,
                get_details=get_details,
                save_to_file=False,
                keyword=keyword
            )
            aggregated_results.extend(keyword_results)
        
        if aggregated_results:
            self.save_results(aggregated_results, suffix="_multi")
        else:
            print("âŒ ëª¨ë“  í‚¤ì›Œë“œì—ì„œ ìœ íš¨í•œ 010 ì „í™”ë²ˆí˜¸ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        
        return aggregated_results

    def build_search_url(self, keyword):
        encoded = quote_plus(keyword)
        return f"https://map.kakao.com/?from=total&nil_suggest=btn&q={encoded}&tab=place"

    def save_results(self, results, suffix=""):
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = self.output_path / f'kakao_phone_stores_{timestamp}{suffix}.csv'
        df = pd.DataFrame(results)
        df.to_csv(filename, index=False, encoding='utf-8-sig')
        print(f"ğŸ’¾ íŒŒì¼ ì €ì¥ ì™„ë£Œ: {filename}")
        
        phone_count = df['ì „í™”ë²ˆí˜¸'].apply(lambda x: isinstance(x, str) and x.startswith('010')).sum()
        addr_count = df['ì£¼ì†Œ'].apply(lambda x: isinstance(x, str) and len(x) > 0).sum()
        
        print(f"\nğŸ“Š í†µê³„:")
        print(f"  - 010 ì „í™”ë²ˆí˜¸ ìˆìŒ: {phone_count}ê°œ")
        print(f"  - ì£¼ì†Œ ìˆìŒ: {addr_count}ê°œ")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    keywords = [
        "íœ´ëŒ€í° ì„±ì§€",
        "íœ´ëŒ€í° íŒë§¤ì ",
        "íœ´ëŒ€í° ë°±í™”ì ",
        "íœ´ëŒ€í° ëŒ€ë¦¬ì ",
        "íœ´ëŒ€í° í• ì¸ì ",
        "íœ´ëŒ€í° ë§¤ì¥",
    ]
    
    print("ğŸš€ ì¹´ì¹´ì˜¤ë§µ í¬ë¡¤ë§ ì‹œì‘...")
    print(f"ğŸ”‘ ëŒ€ìƒ í‚¤ì›Œë“œ: {', '.join(keywords)}\n")
    
    # í¬ë¡¤ëŸ¬ ì‹¤í–‰
    crawler = KakaoMapCrawlerV2(headless=False)  # headless=Trueë¡œ ì„¤ì •í•˜ë©´ ë¸Œë¼ìš°ì € ìˆ¨ê¹€
    results = crawler.crawl_keywords(keywords, get_details=True)
    
    print(f"\nğŸ‰ í¬ë¡¤ë§ ì™„ë£Œ! 010 ë²ˆí˜¸ ë§¤ì¥ {len(results)}ê°œ ìˆ˜ì§‘")


if __name__ == "__main__":
    main()
